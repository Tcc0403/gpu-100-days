{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae381edc",
   "metadata": {},
   "source": [
    "# Fused Softmax\n",
    "Adopted from https://triton-lang.org/main/getting-started/tutorials/02-fused-softmax.html#sphx-glr-getting-started-tutorials-02-fused-softmax-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4870252b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103594a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e0d55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b665eda1",
   "metadata": {},
   "source": [
    "## Naive softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce4bf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_softmax(x):\n",
    "    \"\"\"Compute row-wise softmax of X using native pytorch\n",
    "\n",
    "    We subtract the maximum element in order to avoid overflows. Softmax is invariant to\n",
    "    this shift.\n",
    "    \"\"\"\n",
    "    # read  MN elements ; write M  elements\n",
    "    x_max = x.max(dim=1)[0]\n",
    "    # read MN + M elements ; write MN elements\n",
    "    z = x - x_max[:, None]\n",
    "    # read  MN elements ; write MN elements\n",
    "    numerator = torch.exp(z)\n",
    "    # read  MN elements ; write M  elements\n",
    "    denominator = numerator.sum(dim=1)\n",
    "    # read MN + M elements ; write MN elements\n",
    "    ret = numerator / denominator[:, None]\n",
    "    # in total: read 5MN + 2M elements ; wrote 3MN + 2M elements\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9e48e4",
   "metadata": {},
   "source": [
    "## Test the correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d18b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_softmax(BT, V, f):\n",
    "    shape = (BT, V)\n",
    "    x = torch.randn(shape, device=DEVICE)\n",
    "\n",
    "    output_ref = torch.softmax(x, dim=-1)\n",
    "    output_triton = f(x)    \n",
    "    \n",
    "    print(f\"Testing {shape=}\")\n",
    "    torch.testing.assert_close(output_triton, output_ref)\n",
    "    print(\"âœ… Triton kernel is correct!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446d219c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BT = 1024\n",
    "for V in [2 ** i for i in range(9, 16)]:\n",
    "    try:\n",
    "        test_softmax(BT, V, naive_softmax)\n",
    "    except AssertionError as e:\n",
    "        print(\"AssertionError occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2808aaa",
   "metadata": {},
   "source": [
    "## Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6328f3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=['V'],  # Argument names to use as an x-axis for the plot.\n",
    "        x_vals=[2 ** i for i in range(3, 14, 1)],  # Different possible values for `x_name`.\n",
    "        x_log=True,  # x axis is logarithmic.\n",
    "        line_arg='provider',  # Argument name whose value corresponds to a different line in the plot.\n",
    "        line_vals=['naive', 'torch'],  # Possible values for `line_arg`.\n",
    "        line_names=['naive', 'torch'],  # Label name for the lines.\n",
    "        styles=[('blue', '-'), ('green', '-')],  # Line styles.\n",
    "        ylabel='GB/s',  # Label name for the y-axis.\n",
    "        plot_name='naive-softmax-performance',  # Name for the plot. Used also as a file name for saving the plot.\n",
    "        args={},  # Values for function arguments not in `x_names` and `y_name`.\n",
    "    ))\n",
    "def benchmark(V, provider):\n",
    "    BT = 4096\n",
    "    shape = (BT, V)\n",
    "    x = torch.randn(shape, device=DEVICE, dtype=torch.float32)\n",
    "\n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    if provider == 'torch':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.softmax(x, dim=-1), quantiles=quantiles)\n",
    "    if provider == 'naive':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: naive_softmax(x), quantiles=quantiles)\n",
    "    gbps = lambda ms: 2 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)\n",
    "    return gbps(ms), gbps(max_ms), gbps(min_ms)\n",
    "\n",
    "\n",
    "benchmark.run(print_data=True, show_plots=True, save_path=os.path.abspath(\"../benchmark\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e8674e",
   "metadata": {},
   "source": [
    "## Write Triton kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c010ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def softmax_kernel_v0(x_ptr, output_ptr, n_cols, BLOCK_SIZE: tl.constexpr):\n",
    "    \n",
    "    pid = tl.program_id(0)\n",
    "\n",
    "    # Calculate the starting pointer of each row\n",
    "    x_row_start = x_ptr + pid * n_cols\n",
    "    output_row_start = output_ptr + pid * n_cols\n",
    "\n",
    "    offsets = tl.arange(0, BLOCK_SIZE)\n",
    "    mask = offsets < n_cols\n",
    "\n",
    "    x_row = tl.load(x_row_start + offsets, mask=mask, other=float('-inf')) # shape: (1, BLOCK_SIZE)\n",
    "    x_max = tl.max(x_row, axis=0)                                          # shape: (1,)\n",
    "    numerator = tl.exp(x_row - x_max)                                      # shape: (1, BLOCK_SIZE)\n",
    "    denominator = tl.sum(numerator, axis=0)                                # shape: (1,)\n",
    "    \n",
    "    softmax_output = numerator / denominator                               # shape: (1, BLOCK_SIZE)\n",
    " \n",
    "\n",
    "    tl.store(output_row_start + offsets, softmax_output, mask=mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dacf244",
   "metadata": {},
   "source": [
    "## Helper function to allocate tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115e6846",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triton_softmax_v0(x):\n",
    "    BT, V = x.shape\n",
    "    n_rows, n_cols = BT, V\n",
    "    output = torch.empty_like(x)\n",
    "    \n",
    "    MAX_BLOCK_SIZE = 65536 // x.element_size()\n",
    "    BLOCK_SIZE = min(MAX_BLOCK_SIZE, triton.next_power_of_2(n_cols))\n",
    "\n",
    "    assert n_cols <= BLOCK_SIZE, f\"This implementation does not support more than {BLOCK_SIZE} elements in the last dimension. Got:{n_cols}\"\n",
    "\n",
    "    grid = lambda META: (n_rows,)\n",
    "    softmax_kernel_v0[grid](x, output, n_cols, BLOCK_SIZE=BLOCK_SIZE)\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621ab898",
   "metadata": {},
   "source": [
    "## Benchmark with `triton.testing.do_bench`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3dce90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=['V'],  # Argument names to use as an x-axis for the plot.\n",
    "        x_vals=[2 ** i for i in range(3, 14, 1)],  # Different possible values for `x_name`.\n",
    "        x_log=True,  # x axis is logarithmic.\n",
    "        line_arg='provider',  # Argument name whose value corresponds to a different line in the plot.\n",
    "        line_vals=['triton', 'torch'],  # Possible values for `line_arg`.\n",
    "        line_names=['triton', 'torch'],  # Label name for the lines.\n",
    "        styles=[('blue', '-'), ('green', '-')],  # Line styles.\n",
    "        ylabel='GB/s',  # Label name for the y-axis.\n",
    "        plot_name='softmax-performance',  # Name for the plot. Used also as a file name for saving the plot.\n",
    "        args={},  # Values for function arguments not in `x_names` and `y_name`.\n",
    "    ))\n",
    "def benchmark(V, provider):\n",
    "    BT = 4096\n",
    "    shape = (BT, V)\n",
    "    x = torch.randn(shape, device=DEVICE, dtype=torch.float32)\n",
    "\n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    if provider == 'torch':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.softmax(x, dim=-1), quantiles=quantiles)\n",
    "    if provider == 'triton':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: triton_softmax_v0(x), quantiles=quantiles)\n",
    "    gbps = lambda ms: 2 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)\n",
    "    return gbps(ms), gbps(max_ms), gbps(min_ms)\n",
    "\n",
    "\n",
    "benchmark.run(print_data=True, show_plots=True, save_path=os.path.abspath(\"../benchmark\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5181a6c",
   "metadata": {},
   "source": [
    "## Simple for loop approach\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6558e24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def for_loop_softmax_kernel(x_ptr, output_ptr, n_cols, BLOCK_SIZE: tl.constexpr):\n",
    "    \n",
    "    pid = tl.program_id(0)\n",
    "\n",
    "    # Calculate the starting pointer of each row\n",
    "    x_row_start = x_ptr + pid * n_cols\n",
    "    output_row_start = output_ptr + pid * n_cols\n",
    "\n",
    "\n",
    "    x_max = float(\"-inf\")\n",
    "\n",
    "    # First Pass: Find max_x\n",
    "    for i in tl.range(0, n_cols, BLOCK_SIZE):\n",
    "        # Chunk in row-wise. Update global maximum each loops\n",
    "        offsets = i + tl.arange(0, BLOCK_SIZE)\n",
    "        mask = offsets < n_cols\n",
    "        x_block = tl.load(x_row_start + offsets, mask=mask, other=float('-inf')) \n",
    "        \n",
    "        block_max = tl.max(x_block, axis=0)  # local maximum\n",
    "        x_max = tl.maximum(x_max, block_max)         # update global maximum\n",
    "\n",
    "\n",
    "    # Second Pass: Find denominator sum(exp(x - x_max))\n",
    "    denominator = 0.0\n",
    "\n",
    "    for i in tl.range(0, n_cols, BLOCK_SIZE):\n",
    "        offsets = i + tl.arange(0, BLOCK_SIZE)\n",
    "        mask = offsets < n_cols\n",
    "        x_block = tl.load(x_row_start + offsets, mask=mask, other=float('-inf')) \n",
    "\n",
    "        numerator = tl.exp(x_block - x_max)\n",
    "        denominator += tl.sum(numerator)\n",
    "\n",
    "    # Now we have the correct denominator\n",
    "\n",
    "    # Final Pass: Calculate output and store\n",
    "\n",
    "    for i in tl.range(0, n_cols, BLOCK_SIZE):\n",
    "        offsets = i + tl.arange(0, BLOCK_SIZE)\n",
    "        mask = offsets < n_cols\n",
    "        x_block = tl.load(x_row_start + offsets, mask=mask, other=float('-inf')) \n",
    "\n",
    "        numerator = tl.exp(x_block - x_max)\n",
    "        output_block = numerator / denominator\n",
    "\n",
    "        tl.store(output_row_start + offsets, output_block, mask=mask)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def triton_for_loop_softmax(x):\n",
    "    BT, V = x.shape\n",
    "    n_rows, n_cols = BT, V\n",
    "    output = torch.empty_like(x)\n",
    "    \n",
    "    MAX_BLOCK_SIZE = 65536 // x.element_size()\n",
    "    BLOCK_SIZE = min(MAX_BLOCK_SIZE, triton.next_power_of_2(n_cols))\n",
    "\n",
    "    grid = lambda META: (n_rows,)\n",
    "    for_loop_softmax_kernel[grid](x, output, n_cols, BLOCK_SIZE=BLOCK_SIZE)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca326c7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82e18e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "BT = 1024\n",
    "for V in [2 ** i for i in range(9, 16)]:\n",
    "      test_softmax(BT, V, triton_for_loop_softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "097146df",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     26\u001b[39m     gbps = \u001b[38;5;28;01mlambda\u001b[39;00m ms: \u001b[32m2\u001b[39m * x.numel() * x.element_size() * \u001b[32m1e-9\u001b[39m / (ms * \u001b[32m1e-3\u001b[39m)\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m gbps(ms), gbps(max_ms), gbps(min_ms)\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[43mbenchmark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprint_data\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_plots\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mabspath\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../benchmark\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/tcc/gpu-100-days/day003/.venv/lib/python3.13/site-packages/triton/testing.py:392\u001b[39m, in \u001b[36mMark.run\u001b[39m\u001b[34m(self, show_plots, print_data, save_path, return_df, **kwargs)\u001b[39m\n\u001b[32m    390\u001b[39m     html.write(\u001b[33m\"\u001b[39m\u001b[33m<html><body>\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    391\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m bench \u001b[38;5;129;01min\u001b[39;00m benchmarks:\n\u001b[32m--> \u001b[39m\u001b[32m392\u001b[39m     result_dfs.append(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbench\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_plots\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    393\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m save_path:\n\u001b[32m    394\u001b[39m         html.write(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m<image src=\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mbench.plot_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.png\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m/>\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/tcc/gpu-100-days/day003/.venv/lib/python3.13/site-packages/triton/testing.py:335\u001b[39m, in \u001b[36mMark._run\u001b[39m\u001b[34m(self, bench, save_path, show_plots, print_data, diff_col, save_precision, **kwrags)\u001b[39m\n\u001b[32m    333\u001b[39m row_mean, row_min, row_max = [], [], []\n\u001b[32m    334\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m bench.line_vals:\n\u001b[32m--> \u001b[39m\u001b[32m335\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mx_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43mbench\u001b[49m\u001b[43m.\u001b[49m\u001b[43mline_arg\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mbench\u001b[49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwrags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    336\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    337\u001b[39m         y_mean, y_min, y_max = ret\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mbenchmark\u001b[39m\u001b[34m(V, provider)\u001b[39m\n\u001b[32m     23\u001b[39m     ms, min_ms, max_ms = triton.testing.do_bench(\u001b[38;5;28;01mlambda\u001b[39;00m: torch.softmax(x, dim=-\u001b[32m1\u001b[39m), quantiles=quantiles)\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m provider == \u001b[33m'\u001b[39m\u001b[33mtriton\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     ms, min_ms, max_ms = \u001b[43mtriton\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtesting\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdo_bench\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtriton_for_loop_softmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantiles\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquantiles\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m gbps = \u001b[38;5;28;01mlambda\u001b[39;00m ms: \u001b[32m2\u001b[39m * x.numel() * x.element_size() * \u001b[32m1e-9\u001b[39m / (ms * \u001b[32m1e-3\u001b[39m)\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m gbps(ms), gbps(max_ms), gbps(min_ms)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/tcc/gpu-100-days/day003/.venv/lib/python3.13/site-packages/triton/testing.py:184\u001b[39m, in \u001b[36mdo_bench\u001b[39m\u001b[34m(fn, warmup, rep, grad_to_none, quantiles, return_mode)\u001b[39m\n\u001b[32m    182\u001b[39m     end_event[i].record()\n\u001b[32m    183\u001b[39m \u001b[38;5;66;03m# Record clocks\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m \u001b[43mdi\u001b[49m\u001b[43m.\u001b[49m\u001b[43msynchronize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    185\u001b[39m times = [s.elapsed_time(e) \u001b[38;5;28;01mfor\u001b[39;00m s, e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(start_event, end_event)]\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _summarize_statistics(times, quantiles, return_mode)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/tcc/gpu-100-days/day003/.venv/lib/python3.13/site-packages/torch/cuda/__init__.py:1040\u001b[39m, in \u001b[36msynchronize\u001b[39m\u001b[34m(device)\u001b[39m\n\u001b[32m   1038\u001b[39m _lazy_init()\n\u001b[32m   1039\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.cuda.device(device):\n\u001b[32m-> \u001b[39m\u001b[32m1040\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_cuda_synchronize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=['V'],  # Argument names to use as an x-axis for the plot.\n",
    "        x_vals=[2 ** i for i in range(3, 18, 1)],  # Different possible values for `x_name`.\n",
    "        x_log=True,  # x axis is logarithmic.\n",
    "        line_arg='provider',  # Argument name whose value corresponds to a different line in the plot.\n",
    "        line_vals=['triton', 'torch'],  # Possible values for `line_arg`.\n",
    "        line_names=['triton', 'torch'],  # Label name for the lines.\n",
    "        styles=[('blue', '-'), ('green', '-')],  # Line styles.\n",
    "        ylabel='GB/s',  # Label name for the y-axis.\n",
    "        plot_name='softmax-naive-for-loop-performance',  # Name for the plot. Used also as a file name for saving the plot.\n",
    "        args={},  # Values for function arguments not in `x_names` and `y_name`.\n",
    "    ))\n",
    "def benchmark(V, provider):\n",
    "    BT = 4096\n",
    "    shape = (BT, V)\n",
    "    x = torch.randn(shape, device=DEVICE, dtype=torch.float32)\n",
    "\n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    if provider == 'torch':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.softmax(x, dim=-1), quantiles=quantiles)\n",
    "    if provider == 'triton':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: triton_for_loop_softmax(x), quantiles=quantiles)\n",
    "    gbps = lambda ms: 2 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)\n",
    "    return gbps(ms), gbps(max_ms), gbps(min_ms)\n",
    "\n",
    "\n",
    "benchmark.run(print_data=True, show_plots=True, save_path=os.path.abspath(\"../benchmark\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88751fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_settings(n, element_size):\n",
    "    # reference: https://github.com/unslothai/unsloth/blob/fd753fed99ed5f10ef8a9b7139588d9de9ddecfb/unsloth/kernels/utils.py#L43\n",
    "\n",
    "    MAX_BLOCK_SIZE = 65536 // element_size\n",
    "    BLOCK_SIZE = min(MAX_BLOCK_SIZE, triton.next_power_of_2(n))\n",
    "    num_warps = 4\n",
    "    if BLOCK_SIZE >= 32768 // element_size:\n",
    "        num_warps = 32 \n",
    "    elif BLOCK_SIZE >= 8192 // element_size:\n",
    "        num_warps = 16\n",
    "    elif BLOCK_SIZE >= 2048 // element_size:\n",
    "        num_warps = 8\n",
    "    return BLOCK_SIZE, num_warps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379e2209",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triton_tuned_for_loop_softmax(x):\n",
    "    BT, V = x.shape\n",
    "    n_rows, n_cols = BT, V\n",
    "    output = torch.empty_like(x)\n",
    "    \n",
    "    BLOCK_SIZE, num_warps = calculate_settings(n_cols, x.element_size())\n",
    "\n",
    "    \n",
    "    grid = lambda META: (n_rows,)\n",
    "    for_loop_softmax_kernel[grid](x, output, n_cols, BLOCK_SIZE=BLOCK_SIZE, num_warps=num_warps)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2190d67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=['V'],  # Argument names to use as an x-axis for the plot.\n",
    "        x_vals=[2 ** i for i in range(3, 18, 1)],  # Different possible values for `x_name`.\n",
    "        x_log=True,  # x axis is logarithmic.\n",
    "        line_arg='provider',  # Argument name whose value corresponds to a different line in the plot.\n",
    "        line_vals=['triton', 'torch'],  # Possible values for `line_arg`.\n",
    "        line_names=['triton', 'torch'],  # Label name for the lines.\n",
    "        styles=[('blue', '-'), ('green', '-')],  # Line styles.\n",
    "        ylabel='GB/s',  # Label name for the y-axis.\n",
    "        plot_name='softmax-tuned-for-loop-performance',  # Name for the plot. Used also as a file name for saving the plot.\n",
    "        args={},  # Values for function arguments not in `x_names` and `y_name`.\n",
    "    ))\n",
    "def benchmark(V, provider):\n",
    "    BT = 4096\n",
    "    shape = (BT, V)\n",
    "    x = torch.randn(shape, device=DEVICE, dtype=torch.float32)\n",
    "\n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    if provider == 'torch':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.softmax(x, dim=-1), quantiles=quantiles)\n",
    "    if provider == 'triton':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: triton_tuned_for_loop_softmax(x), quantiles=quantiles)\n",
    "    gbps = lambda ms: 2 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)\n",
    "    return gbps(ms), gbps(max_ms), gbps(min_ms)\n",
    "\n",
    "\n",
    "benchmark.run(print_data=True, show_plots=True, save_path=os.path.abspath(\"../benchmark\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c044192",
   "metadata": {},
   "source": [
    "## Better Alogrithm : Online Softmax \n",
    "\n",
    "https://github.com/NVIDIA/online-softmax\n",
    "\n",
    "https://arxiv.org/pdf/1805.02867"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3b030a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def online_softmax_kernel(x_ptr, output_ptr, n_cols, BLOCK_SIZE: tl.constexpr):\n",
    "    \n",
    "    pid = tl.program_id(0)\n",
    "\n",
    "    # Calculate the starting pointer of each row\n",
    "    x_row_start = x_ptr + pid * n_cols\n",
    "    output_row_start = output_ptr + pid * n_cols\n",
    "\n",
    "    # First Pass: Find statistics maximum m and denominator d.\n",
    "    x_max = float('-inf')\n",
    "    denominator = 0.0\n",
    "\n",
    "    for i in tl.range(0, n_cols, BLOCK_SIZE):\n",
    "        offsets = i + tl.arange(0, BLOCK_SIZE)\n",
    "        mask = offsets < n_cols\n",
    "        x_block = tl.load(x_row_start + offsets, mask=mask, other=float('-inf')) \n",
    "        block_max = tl.max(x_block)\n",
    "\n",
    "        new_x_max = tl.maximum(x_max, block_max)\n",
    "        denominator = denominator * tl.exp(x_max - new_x_max) + tl.sum(tl.exp(x_block - new_x_max))\n",
    "        x_max = new_x_max\n",
    "\n",
    "\n",
    "    # Now we have the correct denominator\n",
    "\n",
    "    # Final Pass: Calculate output and store\n",
    "\n",
    "    for i in tl.range(0, n_cols, BLOCK_SIZE):\n",
    "        offsets = i + tl.arange(0, BLOCK_SIZE)\n",
    "        mask = offsets < n_cols\n",
    "        x_block = tl.load(x_row_start + offsets, mask=mask, other=float('-inf')) \n",
    "\n",
    "        numerator = tl.exp(x_block - x_max)\n",
    "        output_block = numerator / denominator\n",
    "\n",
    "        tl.store(output_row_start + offsets, output_block, mask=mask)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def triton_online_softmax(x):\n",
    "    BT, V = x.shape\n",
    "    n_rows, n_cols = BT, V\n",
    "    output = torch.empty_like(x)\n",
    "\n",
    "    BLOCK_SIZE, num_warps = calculate_settings(n_cols, x.element_size())\n",
    "\n",
    "    grid = lambda META: (n_rows,)\n",
    "    online_softmax_kernel[grid](x, output, n_cols, BLOCK_SIZE=BLOCK_SIZE, num_warps=num_warps)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04065bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BT = 1024\n",
    "for V in [2 ** i for i in range(9, 16)]:\n",
    "      test_softmax(BT, V, triton_online_softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ee044b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=['V'],  # Argument names to use as an x-axis for the plot.\n",
    "        x_vals=[2 ** i for i in range(3, 18, 1)],  # Different possible values for `x_name`.\n",
    "        x_log=True,  # x axis is logarithmic.\n",
    "        line_arg='provider',  # Argument name whose value corresponds to a different line in the plot.\n",
    "        line_vals=['triton', 'torch'],  # Possible values for `line_arg`.\n",
    "        line_names=['triton', 'torch'],  # Label name for the lines.\n",
    "        styles=[('blue', '-'), ('green', '-')],  # Line styles.\n",
    "        ylabel='GB/s',  # Label name for the y-axis.\n",
    "        plot_name='online-softmax-performance',  # Name for the plot. Used also as a file name for saving the plot.\n",
    "        args={},  # Values for function arguments not in `x_names` and `y_name`.\n",
    "    ))\n",
    "def benchmark(V, provider):\n",
    "    BT = 4096\n",
    "    shape = (BT, V)\n",
    "    x = torch.randn(shape, device=DEVICE, dtype=torch.float32)\n",
    "\n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    if provider == 'torch':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.softmax(x, dim=-1), quantiles=quantiles)\n",
    "    if provider == 'triton':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: triton_online_softmax(x), quantiles=quantiles)\n",
    "    gbps = lambda ms: 2 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)\n",
    "    return gbps(ms), gbps(max_ms), gbps(min_ms)\n",
    "\n",
    "\n",
    "benchmark.run(print_data=True, show_plots=True, save_path=os.path.abspath(\"../benchmark\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164574b1",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fd40a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=['V'],  # Argument names to use as an x-axis for the plot.\n",
    "        x_vals=[2 ** i for i in range(3, 18, 1)],  # Different possible values for `x_name`.\n",
    "        x_log=True,  # x axis is logarithmic.\n",
    "        line_arg='provider',  # Argument name whose value corresponds to a different line in the plot.\n",
    "        line_vals=['triton_online_softmax', 'triton_naive_softmax', 'torch'],  # Possible values for `line_arg`.\n",
    "        line_names=['triton_online_softmax', 'triton_naive_softmax', 'torch'],  # Label name for the lines.\n",
    "        styles=[('red', '-'), ('blue', '-'), ('green', '-')],  # Line styles.\n",
    "        ylabel='GB/s',  # Label name for the y-axis.\n",
    "        plot_name='all-softmax-performance',  # Name for the plot. Used also as a file name for saving the plot.\n",
    "        args={},  # Values for function arguments not in `x_names` and `y_name`.\n",
    "    ))\n",
    "def benchmark(V, provider):\n",
    "    BT = 4096\n",
    "    shape = (BT, V)\n",
    "    x = torch.randn(shape, device=DEVICE, dtype=torch.float32)\n",
    "\n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    if provider == 'torch':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.softmax(x, dim=-1), quantiles=quantiles)\n",
    "    if provider == 'triton_online_softmax':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: triton_online_softmax(x), quantiles=quantiles)\n",
    "    if provider == 'triton_naive_softmax':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: triton_tuned_for_loop_softmax(x), quantiles=quantiles)\n",
    "    gbps = lambda ms: 2 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)\n",
    "    return gbps(ms), gbps(max_ms), gbps(min_ms)\n",
    "\n",
    "\n",
    "benchmark.run(print_data=True, show_plots=True, save_path=os.path.abspath(\"../benchmark\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
