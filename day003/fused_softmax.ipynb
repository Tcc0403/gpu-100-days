{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae381edc",
   "metadata": {},
   "source": [
    "# Fused Softmax\n",
    "Adopted from https://triton-lang.org/main/getting-started/tutorials/02-fused-softmax.html#sphx-glr-getting-started-tutorials-02-fused-softmax-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4870252b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103594a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e0d55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b665eda1",
   "metadata": {},
   "source": [
    "## Naive softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce4bf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_softmax(x):\n",
    "    \"\"\"Compute row-wise softmax of X using native pytorch\n",
    "\n",
    "    We subtract the maximum element in order to avoid overflows. Softmax is invariant to\n",
    "    this shift.\n",
    "    \"\"\"\n",
    "    # read  MN elements ; write M  elements\n",
    "    x_max = x.max(dim=1)[0]\n",
    "    # read MN + M elements ; write MN elements\n",
    "    z = x - x_max[:, None]\n",
    "    # read  MN elements ; write MN elements\n",
    "    numerator = torch.exp(z)\n",
    "    # read  MN elements ; write M  elements\n",
    "    denominator = numerator.sum(dim=1)\n",
    "    # read MN + M elements ; write MN elements\n",
    "    ret = numerator / denominator[:, None]\n",
    "    # in total: read 5MN + 2M elements ; wrote 3MN + 2M elements\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9e48e4",
   "metadata": {},
   "source": [
    "## Test the correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d18b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_softmax(BT, V, f):\n",
    "    shape = (BT, V)\n",
    "    x = torch.randn(shape, device=DEVICE)\n",
    "\n",
    "    output_ref = torch.softmax(x, dim=-1)\n",
    "    output_triton = f(x)    \n",
    "    \n",
    "    print(f\"Testing {shape=}\")\n",
    "    torch.testing.assert_close(output_triton, output_ref)\n",
    "    print(\"âœ… Triton kernel is correct!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446d219c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BT = 1024\n",
    "for V in [2 ** i for i in range(9, 16)]:\n",
    "    try:\n",
    "        test_softmax(BT, V, naive_softmax)\n",
    "    except AssertionError as e:\n",
    "        print(\"AssertionError occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2808aaa",
   "metadata": {},
   "source": [
    "## Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6328f3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=['V'],  # Argument names to use as an x-axis for the plot.\n",
    "        x_vals=[2 ** i for i in range(3, 14, 1)],  # Different possible values for `x_name`.\n",
    "        x_log=True,  # x axis is logarithmic.\n",
    "        line_arg='provider',  # Argument name whose value corresponds to a different line in the plot.\n",
    "        line_vals=['naive', 'torch'],  # Possible values for `line_arg`.\n",
    "        line_names=['naive', 'torch'],  # Label name for the lines.\n",
    "        styles=[('blue', '-'), ('green', '-')],  # Line styles.\n",
    "        ylabel='GB/s',  # Label name for the y-axis.\n",
    "        plot_name='naive-softmax-performance',  # Name for the plot. Used also as a file name for saving the plot.\n",
    "        args={},  # Values for function arguments not in `x_names` and `y_name`.\n",
    "    ))\n",
    "def benchmark(V, provider):\n",
    "    BT = 4096\n",
    "    shape = (BT, V)\n",
    "    x = torch.randn(shape, device=DEVICE, dtype=torch.float32)\n",
    "\n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    if provider == 'torch':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.softmax(x, dim=-1), quantiles=quantiles)\n",
    "    if provider == 'naive':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: naive_softmax(x), quantiles=quantiles)\n",
    "    gbps = lambda ms: 2 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)\n",
    "    return gbps(ms), gbps(max_ms), gbps(min_ms)\n",
    "\n",
    "\n",
    "benchmark.run(print_data=True, show_plots=True, save_path=os.path.abspath(\"../benchmark\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e8674e",
   "metadata": {},
   "source": [
    "## Write Triton kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c010ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def softmax_kernel_v0(x_ptr, output_ptr, n_cols, BLOCK_SIZE: tl.constexpr):\n",
    "    \n",
    "    pid = tl.program_id(0)\n",
    "\n",
    "    # Calculate the starting pointer of each row\n",
    "    x_row_start = x_ptr + pid * n_cols\n",
    "    output_row_start = output_ptr + pid * n_cols\n",
    "\n",
    "    offsets = tl.arange(0, BLOCK_SIZE)\n",
    "    mask = offsets < n_cols\n",
    "\n",
    "    x_row = tl.load(x_row_start + offsets, mask=mask, other=float('-inf')) # shape: (1, BLOCK_SIZE)\n",
    "    x_max = tl.max(x_row, axis=0)                                          # shape: (1,)\n",
    "    numerator = tl.exp(x_row - x_max)                                      # shape: (1, BLOCK_SIZE)\n",
    "    denominator = tl.sum(numerator, axis=0)                                # shape: (1,)\n",
    "    \n",
    "    softmax_output = numerator / denominator                               # shape: (1, BLOCK_SIZE)\n",
    " \n",
    "\n",
    "    tl.store(output_row_start + offsets, softmax_output, mask=mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dacf244",
   "metadata": {},
   "source": [
    "## Helper function to allocate tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115e6846",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triton_softmax_v0(x):\n",
    "    BT, V = x.shape\n",
    "    n_rows, n_cols = BT, V\n",
    "    output = torch.empty_like(x)\n",
    "    \n",
    "    MAX_BLOCK_SIZE = 65536 // x.element_size()\n",
    "    BLOCK_SIZE = min(MAX_BLOCK_SIZE, triton.next_power_of_2(n_cols))\n",
    "\n",
    "    assert n_cols <= BLOCK_SIZE, f\"This implementation does not support more than {BLOCK_SIZE} elements in the last dimension. Got:{n_cols}\"\n",
    "\n",
    "    grid = lambda META: (n_rows,)\n",
    "    softmax_kernel_v0[grid](x, output, n_cols, BLOCK_SIZE=BLOCK_SIZE)\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621ab898",
   "metadata": {},
   "source": [
    "## Benchmark with `triton.testing.do_bench`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3dce90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=['V'],  # Argument names to use as an x-axis for the plot.\n",
    "        x_vals=[2 ** i for i in range(3, 14, 1)],  # Different possible values for `x_name`.\n",
    "        x_log=True,  # x axis is logarithmic.\n",
    "        line_arg='provider',  # Argument name whose value corresponds to a different line in the plot.\n",
    "        line_vals=['triton', 'torch'],  # Possible values for `line_arg`.\n",
    "        line_names=['triton', 'torch'],  # Label name for the lines.\n",
    "        styles=[('blue', '-'), ('green', '-')],  # Line styles.\n",
    "        ylabel='GB/s',  # Label name for the y-axis.\n",
    "        plot_name='softmax-performance',  # Name for the plot. Used also as a file name for saving the plot.\n",
    "        args={},  # Values for function arguments not in `x_names` and `y_name`.\n",
    "    ))\n",
    "def benchmark(V, provider):\n",
    "    BT = 4096\n",
    "    shape = (BT, V)\n",
    "    x = torch.randn(shape, device=DEVICE, dtype=torch.float32)\n",
    "\n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    if provider == 'torch':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.softmax(x, dim=-1), quantiles=quantiles)\n",
    "    if provider == 'triton':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: triton_softmax_v0(x), quantiles=quantiles)\n",
    "    gbps = lambda ms: 2 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)\n",
    "    return gbps(ms), gbps(max_ms), gbps(min_ms)\n",
    "\n",
    "\n",
    "benchmark.run(print_data=True, show_plots=True, save_path=os.path.abspath(\"../benchmark\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5181a6c",
   "metadata": {},
   "source": [
    "## Simple for loop approach\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6558e24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def for_loop_softmax_kernel(x_ptr, output_ptr, n_cols, BLOCK_SIZE: tl.constexpr):\n",
    "    \n",
    "    pid = tl.program_id(0)\n",
    "\n",
    "    # Calculate the starting pointer of each row\n",
    "    x_row_start = x_ptr + pid * n_cols\n",
    "    output_row_start = output_ptr + pid * n_cols\n",
    "\n",
    "\n",
    "    x_max = float(\"-inf\")\n",
    "\n",
    "    # First Pass: Find max_x\n",
    "    for i in tl.range(0, n_cols, BLOCK_SIZE):\n",
    "        # Chunk in row-wise. Update global maximum each loops\n",
    "        offsets = i + tl.arange(0, BLOCK_SIZE)\n",
    "        mask = offsets < n_cols\n",
    "        x_block = tl.load(x_row_start + offsets, mask=mask, other=float('-inf')) \n",
    "        \n",
    "        block_max = tl.max(x_block, axis=0)  # local maximum\n",
    "        x_max = tl.maximum(x_max, block_max)         # update global maximum\n",
    "\n",
    "\n",
    "    # Second Pass: Find denominator sum(exp(x - x_max))\n",
    "    denominator = 0.0\n",
    "\n",
    "    for i in tl.range(0, n_cols, BLOCK_SIZE):\n",
    "        offsets = i + tl.arange(0, BLOCK_SIZE)\n",
    "        mask = offsets < n_cols\n",
    "        x_block = tl.load(x_row_start + offsets, mask=mask, other=float('-inf')) \n",
    "\n",
    "        numerator = tl.exp(x_block - x_max)\n",
    "        denominator += tl.sum(numerator)\n",
    "\n",
    "    # Now we have the correct denominator\n",
    "\n",
    "    # Final Pass: Calculate output and store\n",
    "\n",
    "    for i in tl.range(0, n_cols, BLOCK_SIZE):\n",
    "        offsets = i + tl.arange(0, BLOCK_SIZE)\n",
    "        mask = offsets < n_cols\n",
    "        x_block = tl.load(x_row_start + offsets, mask=mask, other=float('-inf')) \n",
    "\n",
    "        numerator = tl.exp(x_block - x_max)\n",
    "        output_block = numerator / denominator\n",
    "\n",
    "        tl.store(output_row_start + offsets, output_block, mask=mask)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def triton_for_loop_softmax(x):\n",
    "    BT, V = x.shape\n",
    "    n_rows, n_cols = BT, V\n",
    "    output = torch.empty_like(x)\n",
    "    \n",
    "    MAX_BLOCK_SIZE = 65536 // x.element_size()\n",
    "    BLOCK_SIZE = min(MAX_BLOCK_SIZE, triton.next_power_of_2(n_cols))\n",
    "\n",
    "    grid = lambda META: (n_rows,)\n",
    "    for_loop_softmax_kernel[grid](x, output, n_cols, BLOCK_SIZE=BLOCK_SIZE)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca326c7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82e18e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "BT = 1024\n",
    "for V in [2 ** i for i in range(9, 16)]:\n",
    "      test_softmax(BT, V, triton_for_loop_softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097146df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=['V'],  # Argument names to use as an x-axis for the plot.\n",
    "        x_vals=[2 ** i for i in range(3, 18, 1)],  # Different possible values for `x_name`.\n",
    "        x_log=True,  # x axis is logarithmic.\n",
    "        line_arg='provider',  # Argument name whose value corresponds to a different line in the plot.\n",
    "        line_vals=['triton', 'torch'],  # Possible values for `line_arg`.\n",
    "        line_names=['triton', 'torch'],  # Label name for the lines.\n",
    "        styles=[('blue', '-'), ('green', '-')],  # Line styles.\n",
    "        ylabel='GB/s',  # Label name for the y-axis.\n",
    "        plot_name='softmax-naive-for-loop-performance',  # Name for the plot. Used also as a file name for saving the plot.\n",
    "        args={},  # Values for function arguments not in `x_names` and `y_name`.\n",
    "    ))\n",
    "def benchmark(V, provider):\n",
    "    BT = 4096\n",
    "    shape = (BT, V)\n",
    "    x = torch.randn(shape, device=DEVICE, dtype=torch.float32)\n",
    "\n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    if provider == 'torch':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.softmax(x, dim=-1), quantiles=quantiles)\n",
    "    if provider == 'triton':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: triton_for_loop_softmax(x), quantiles=quantiles)\n",
    "    gbps = lambda ms: 2 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)\n",
    "    return gbps(ms), gbps(max_ms), gbps(min_ms)\n",
    "\n",
    "\n",
    "benchmark.run(print_data=True, show_plots=True, save_path=os.path.abspath(\"../benchmark\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88751fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_settings(n, element_size):\n",
    "    # reference: https://github.com/unslothai/unsloth/blob/fd753fed99ed5f10ef8a9b7139588d9de9ddecfb/unsloth/kernels/utils.py#L43\n",
    "\n",
    "    MAX_BLOCK_SIZE = 65536 // element_size\n",
    "    BLOCK_SIZE = min(MAX_BLOCK_SIZE, triton.next_power_of_2(n))\n",
    "    num_warps = 4\n",
    "    if BLOCK_SIZE >= 32768 // element_size:\n",
    "        num_warps = 32 \n",
    "    elif BLOCK_SIZE >= 8192 // element_size:\n",
    "        num_warps = 16\n",
    "    elif BLOCK_SIZE >= 2048 // element_size:\n",
    "        num_warps = 8\n",
    "    return BLOCK_SIZE, num_warps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379e2209",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triton_tuned_for_loop_softmax(x):\n",
    "    BT, V = x.shape\n",
    "    n_rows, n_cols = BT, V\n",
    "    output = torch.empty_like(x)\n",
    "    \n",
    "    BLOCK_SIZE, num_warps = calculate_settings(n_cols, x.element_size())\n",
    "\n",
    "    \n",
    "    grid = lambda META: (n_rows,)\n",
    "    for_loop_softmax_kernel[grid](x, output, n_cols, BLOCK_SIZE=BLOCK_SIZE, num_warps=num_warps)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2190d67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=['V'],  # Argument names to use as an x-axis for the plot.\n",
    "        x_vals=[2 ** i for i in range(3, 18, 1)],  # Different possible values for `x_name`.\n",
    "        x_log=True,  # x axis is logarithmic.\n",
    "        line_arg='provider',  # Argument name whose value corresponds to a different line in the plot.\n",
    "        line_vals=['triton', 'torch'],  # Possible values for `line_arg`.\n",
    "        line_names=['triton', 'torch'],  # Label name for the lines.\n",
    "        styles=[('blue', '-'), ('green', '-')],  # Line styles.\n",
    "        ylabel='GB/s',  # Label name for the y-axis.\n",
    "        plot_name='softmax-tuned-for-loop-performance',  # Name for the plot. Used also as a file name for saving the plot.\n",
    "        args={},  # Values for function arguments not in `x_names` and `y_name`.\n",
    "    ))\n",
    "def benchmark(V, provider):\n",
    "    BT = 4096\n",
    "    shape = (BT, V)\n",
    "    x = torch.randn(shape, device=DEVICE, dtype=torch.float32)\n",
    "\n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    if provider == 'torch':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.softmax(x, dim=-1), quantiles=quantiles)\n",
    "    if provider == 'triton':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: triton_tuned_for_loop_softmax(x), quantiles=quantiles)\n",
    "    gbps = lambda ms: 2 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)\n",
    "    return gbps(ms), gbps(max_ms), gbps(min_ms)\n",
    "\n",
    "\n",
    "benchmark.run(print_data=True, show_plots=True, save_path=os.path.abspath(\"../benchmark\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c044192",
   "metadata": {},
   "source": [
    "## Better Alogrithm : Online Softmax \n",
    "\n",
    "https://github.com/NVIDIA/online-softmax\n",
    "\n",
    "https://arxiv.org/pdf/1805.02867"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3b030a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def online_softmax_kernel(x_ptr, output_ptr, n_cols, BLOCK_SIZE: tl.constexpr):\n",
    "    \n",
    "    pid = tl.program_id(0)\n",
    "\n",
    "    # Calculate the starting pointer of each row\n",
    "    x_row_start = x_ptr + pid * n_cols\n",
    "    output_row_start = output_ptr + pid * n_cols\n",
    "\n",
    "    # First Pass: Find statistics maximum m and denominator d.\n",
    "    x_max = float('-inf')\n",
    "    denominator = 0.0\n",
    "\n",
    "    for i in tl.range(0, n_cols, BLOCK_SIZE):\n",
    "        offsets = i + tl.arange(0, BLOCK_SIZE)\n",
    "        mask = offsets < n_cols\n",
    "        x_block = tl.load(x_row_start + offsets, mask=mask, other=float('-inf')) \n",
    "        block_max = tl.max(x_block)\n",
    "\n",
    "        new_x_max = tl.maximum(x_max, block_max)\n",
    "        denominator = denominator * tl.exp(x_max - new_x_max) + tl.sum(tl.exp(x_block - new_x_max))\n",
    "        x_max = new_x_max\n",
    "\n",
    "\n",
    "    # Now we have the correct denominator\n",
    "\n",
    "    # Final Pass: Calculate output and store\n",
    "\n",
    "    for i in tl.range(0, n_cols, BLOCK_SIZE):\n",
    "        offsets = i + tl.arange(0, BLOCK_SIZE)\n",
    "        mask = offsets < n_cols\n",
    "        x_block = tl.load(x_row_start + offsets, mask=mask, other=float('-inf')) \n",
    "\n",
    "        numerator = tl.exp(x_block - x_max)\n",
    "        output_block = numerator / denominator\n",
    "\n",
    "        tl.store(output_row_start + offsets, output_block, mask=mask)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def triton_online_softmax(x):\n",
    "    BT, V = x.shape\n",
    "    n_rows, n_cols = BT, V\n",
    "    output = torch.empty_like(x)\n",
    "\n",
    "    BLOCK_SIZE, num_warps = calculate_settings(n_cols, x.element_size())\n",
    "\n",
    "    grid = lambda META: (n_rows,)\n",
    "    online_softmax_kernel[grid](x, output, n_cols, BLOCK_SIZE=BLOCK_SIZE, num_warps=num_warps)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04065bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BT = 1024\n",
    "for V in [2 ** i for i in range(9, 16)]:\n",
    "      test_softmax(BT, V, triton_online_softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ee044b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=['V'],  # Argument names to use as an x-axis for the plot.\n",
    "        x_vals=[2 ** i for i in range(3, 18, 1)],  # Different possible values for `x_name`.\n",
    "        x_log=True,  # x axis is logarithmic.\n",
    "        line_arg='provider',  # Argument name whose value corresponds to a different line in the plot.\n",
    "        line_vals=['triton', 'torch'],  # Possible values for `line_arg`.\n",
    "        line_names=['triton', 'torch'],  # Label name for the lines.\n",
    "        styles=[('blue', '-'), ('green', '-')],  # Line styles.\n",
    "        ylabel='GB/s',  # Label name for the y-axis.\n",
    "        plot_name='online-softmax-performance',  # Name for the plot. Used also as a file name for saving the plot.\n",
    "        args={},  # Values for function arguments not in `x_names` and `y_name`.\n",
    "    ))\n",
    "def benchmark(V, provider):\n",
    "    BT = 4096\n",
    "    shape = (BT, V)\n",
    "    x = torch.randn(shape, device=DEVICE, dtype=torch.float32)\n",
    "\n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    if provider == 'torch':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.softmax(x, dim=-1), quantiles=quantiles)\n",
    "    if provider == 'triton':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: triton_online_softmax(x), quantiles=quantiles)\n",
    "    gbps = lambda ms: 2 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)\n",
    "    return gbps(ms), gbps(max_ms), gbps(min_ms)\n",
    "\n",
    "\n",
    "benchmark.run(print_data=True, show_plots=True, save_path=os.path.abspath(\"../benchmark\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164574b1",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fd40a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=['V'],  # Argument names to use as an x-axis for the plot.\n",
    "        x_vals=[2 ** i for i in range(3, 18, 1)],  # Different possible values for `x_name`.\n",
    "        x_log=True,  # x axis is logarithmic.\n",
    "        line_arg='provider',  # Argument name whose value corresponds to a different line in the plot.\n",
    "        line_vals=['triton_online_softmax', 'triton_naive_softmax', 'torch'],  # Possible values for `line_arg`.\n",
    "        line_names=['triton_online_softmax', 'triton_naive_softmax', 'torch'],  # Label name for the lines.\n",
    "        styles=[('red', '-'), ('blue', '-'), ('green', '-')],  # Line styles.\n",
    "        ylabel='GB/s',  # Label name for the y-axis.\n",
    "        plot_name='all-softmax-performance',  # Name for the plot. Used also as a file name for saving the plot.\n",
    "        args={},  # Values for function arguments not in `x_names` and `y_name`.\n",
    "    ))\n",
    "def benchmark(V, provider):\n",
    "    BT = 4096\n",
    "    shape = (BT, V)\n",
    "    x = torch.randn(shape, device=DEVICE, dtype=torch.float32)\n",
    "\n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    if provider == 'torch':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.softmax(x, dim=-1), quantiles=quantiles)\n",
    "    if provider == 'triton_online_softmax':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: triton_online_softmax(x), quantiles=quantiles)\n",
    "    if provider == 'triton_naive_softmax':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: triton_tuned_for_loop_softmax(x), quantiles=quantiles)\n",
    "    gbps = lambda ms: 2 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)\n",
    "    return gbps(ms), gbps(max_ms), gbps(min_ms)\n",
    "\n",
    "\n",
    "benchmark.run(print_data=True, show_plots=True, save_path=os.path.abspath(\"../benchmark\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
